# Copy this file, remove ".example", and make changes like:
# replace ~/code/pyspark_aws_etl below by location of your pyspark_aws_etl repo.
# add : "-v ~/path/to/your/repo/pipelines:/mnt/external_pipelines \" if putting the pipelines in an external repo.
# add : "-v ~/.aws:/.aws \" to use the tool to run jobs in AWS (ad-hoc or scheduled). Requires awcli setup on host (with ~/.aws setup with profile "default").
# other options to be added if necessary:
# --cpu-shares \
# --cpus 6 \
# --memory 6g \


pyspark_aws_etl_jobs_home=/path/to/yaetos_jobs  # Fill path here

#cd $pyspark_aws_etl_home
docker build -t pyspark_container . # builds from Dockerfile
docker run -it -p 4040:4040 -p 8080:8080 -p 8081:8081 \
    -v $pyspark_aws_etl_jobs_home:/mnt/external_pipelines \
    -v $HOME/.aws:/.aws \
    -h spark \
    -w /mnt/external_pipelines/ \
    pyspark_container
